# -*- coding: utf-8 -*-
import pytorch_lightning as pl
import torch
from torch import nn
import torch.nn.functional as F
from torch.autograd import Function
import torchmetrics
import pathlib
import os
import json

from .modules.brep_encoder import BrepEncoder
from .modules.module_utils.macro import *
from .modules.domain_adv.grl import WarmStartGradientReverseLayer


class NonLinearClassifier(nn.Module):
    def __init__(self, input_dim, num_classes, dropout=0.3):
        """
        A 3-layer MLP with linear outputs
        Args:
            input_dim (int): Dimension of the input tensor
            num_classes (int): Dimension of the output logits
            dropout (float, optional): Dropout used after each linear layer. Defaults to 0.3.
        """
        super().__init__()

        self.linear1 = nn.Linear(input_dim, 512, bias=False)
        self.bn1 = nn.BatchNorm1d(512)
        self.dp1 = nn.Dropout(p=dropout)
        self.linear2 = nn.Linear(512, 512, bias=False)
        self.bn2 = nn.BatchNorm1d(512)
        self.dp2 = nn.Dropout(p=dropout)
        self.linear3 = nn.Linear(512, 256, bias=False)
        self.bn3 = nn.BatchNorm1d(256)
        self.dp3 = nn.Dropout(p=dropout)
        self.linear4 = nn.Linear(256, num_classes)

        for m in self.modules():
            self.weights_init(m)

    def weights_init(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.kaiming_uniform_(m.weight.data)
            if m.bias is not None:
                m.bias.data.fill_(0.0)

    def forward(self, inp, return_feat=False):
        """
        Forward pass
        Args:
            inp (torch.tensor): Inputs features to be mapped to logits
                                (batch_size x input_dim)

        Returns:
            torch.tensor: Logits (batch_size x num_classes)
        """
        feat = inp

        x = F.relu(self.bn1(self.linear1(inp)))
        x = self.dp1(x)
        x = F.relu(self.bn2(self.linear2(x)))
        x = self.dp2(x)
        x = F.relu(self.bn3(self.linear3(x)))
        x = self.dp3(x)
        x = self.linear4(x)
        x = F.softmax(x, dim=-1)

        if return_feat:
            return x, feat
        else:
            return x


class LinearClassifier(nn.Module):
    def __init__(self, in_dim, out_dim, bottle_neck_dim=256):
        super().__init__()
        self.bottleneck = nn.Linear(in_dim, bottle_neck_dim)
        self.fc = nn.Linear(bottle_neck_dim, out_dim)
        self.main = nn.Sequential(
            self.bottleneck,
            nn.Sequential(
                nn.LeakyReLU(0.2, inplace=True),
                self.fc
            ),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        for module in self.main.children():
            x = module(x)
        return x


class MultiClassifier(nn.Module):
    def __init__(self, in_dim, num_classes):
        super().__init__()
        self.n = num_classes

        def f():
            return nn.Sequential(
                nn.Linear(in_dim, 256),
                nn.BatchNorm1d(256),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Linear(256, 1),
                nn.Sigmoid()
            )

        for i in range(num_classes):
            self.__setattr__('discriminator_%04d' % i, f())

    def forward(self, x):
        outs = [self.__getattr__('discriminator_%04d' % i)(x) for i in range(self.n)]
        return torch.cat(outs, dim=-1)


class Discriminator(nn.Module):
    def __init__(self, in_dim):
        super().__init__()

        self.grl = WarmStartGradientReverseLayer(alpha=1., lo=0., hi=1., max_iters=1000, auto_step=True)

        self.ad_layer1 = nn.Linear(in_dim, 1024)
        self.ad_layer2 = nn.Linear(1024, 1024)
        self.ad_layer3 = nn.Linear(1024, 1)
        self.sigmoid = nn.Sigmoid()
        self.main = nn.Sequential(
            self.ad_layer1,
            nn.BatchNorm1d(1024),
            nn.LeakyReLU(0.2, inplace=True),
            self.ad_layer2,
            nn.BatchNorm1d(1024),
            nn.LeakyReLU(0.2, inplace=True),
            self.ad_layer3,
            self.sigmoid
        )

    def forward(self, x):
        x = self.grl(x)
        for module in self.main.children():
            x = module(x)
        return x


def CrossEntropyLoss(label, predict_prob, class_level_weight=None, instance_level_weight=None, epsilon=1e-12):
    N, C = label.size()
    N_, C_ = predict_prob.size()
    assert N == N_ and C == C_, 'fatal error: dimension mismatch!'

    if class_level_weight is None:
        class_level_weight = 1.0
    else:
        if len(class_level_weight.size()) == 1:
            class_level_weight = class_level_weight.view(1, class_level_weight.size(0))
        assert class_level_weight.size(1) == C, 'fatal error: dimension mismatch!'

    if instance_level_weight is None:
        instance_level_weight = 1.0
    else:
        if len(instance_level_weight.size()) == 1:
            instance_level_weight = instance_level_weight.view(instance_level_weight.size(0), 1)
        assert instance_level_weight.size(0) == N, 'fatal error: dimension mismatch!'

    ce = -label * torch.log(predict_prob + epsilon)
    return torch.sum(instance_level_weight * ce * class_level_weight) / float(N)


def EntropyLoss(predict_prob, class_level_weight=None, instance_level_weight=None, epsilon=1e-20):
    N, C = predict_prob.size()

    if class_level_weight is None:
        class_level_weight = 1.0
    else:
        if len(class_level_weight.size()) == 1:
            class_level_weight = class_level_weight.view(1, class_level_weight.size(0))
        assert class_level_weight.size(1) == C, 'fatal error: dimension mismatch!'

    if instance_level_weight is None:
        instance_level_weight = 1.0
    else:
        if len(instance_level_weight.size()) == 1:
            instance_level_weight = instance_level_weight.view(instance_level_weight.size(0), 1)
        assert instance_level_weight.size(0) == N, 'fatal error: dimension mismatch!'

    entropy = -predict_prob * torch.log(predict_prob + epsilon)
    return torch.sum(instance_level_weight * entropy * class_level_weight) / float(N)


def BCELossForMultiClassification(label, predict_prob, class_level_weight=None, instance_level_weight=None, epsilon=1e-12):
    N, C = label.size()
    N_, C_ = predict_prob.size()
    assert N == N_ and C == C_, 'fatal error: dimension mismatch!'

    if class_level_weight is None:
        class_level_weight = 1.0
    else:
        if len(class_level_weight.size()) == 1:
            class_level_weight = class_level_weight.view(1, class_level_weight.size(0))
        assert class_level_weight.size(1) == C, 'fatal error: dimension mismatch!'

    if instance_level_weight is None:
        instance_level_weight = 1.0
    else:
        if len(instance_level_weight.size()) == 1:
            instance_level_weight = instance_level_weight.view(instance_level_weight.size(0), 1)
        assert instance_level_weight.size(0) == N, 'fatal error: dimension mismatch!'

    bce = -label * torch.log(predict_prob + epsilon) - (1.0 - label) * torch.log(1.0 - predict_prob + epsilon)
    return torch.sum(instance_level_weight * bce * class_level_weight) / float(N)


class STADomainAdapt(pl.LightningModule):
    """
    PyTorch Lightning module to train/test the model.
    """
    def __init__(self, args):
        super().__init__()
        self.save_hyperparameters()

        # garph encoder--------------------------------------------------------------------
        self.brep_encoder = BrepEncoder(
            # < for graphormer
            num_degree=128,  # number of in degree types in the graph
            num_spatial=64,  # number of spatial types in the graph
            num_edge_dis=64,  # number of edge dis types in the graph
            edge_type="multi_hop",  # edge type in the graph "multi_hop"
            multi_hop_max_dist=16,  # max distance of multi-hop edges
            # >
            num_encoder_layers=args.n_layers_encode,  # num encoder layers
            embedding_dim=args.dim_node,  # encoder embedding dimension
            ffn_embedding_dim=args.d_model,  # encoder embedding dimension for FFN
            num_attention_heads=args.n_heads,  # num encoder attention heads
            dropout=args.dropout,  # dropout probability
            attention_dropout=args.attention_dropout,  # dropout probability for"attention weights"
            activation_dropout=args.act_dropout,  # dropout probability after"activation in FFN"
            layerdrop=0.1,
            encoder_normalize_before=True,  # apply layernorm before each encoder block
            pre_layernorm=True,
            # apply layernorm before self-attention and ffn. Without this, post layernorm will used
            apply_params_init=True,  # use custom param initialization for Graphormer
            activation_fn="gelu",  # activation function to use
        )
        # garph encoder--------------------------------------------------------------------

        # node classifier------------------------------------------------------------------
        self.num_classes = args.num_classes
        self.classifier = NonLinearClassifier(2*args.dim_node, args.num_classes + 1, args.dropout)
        # node classifier------------------------------------------------------------------

        # known-unknown classifier----------------------------------------------------------
        self.discriminator_t = LinearClassifier(2*args.dim_node, 2)

        # multi classes classifier----------------------------------------------------------
        self.discriminator_p = MultiClassifier(2*args.dim_node, args.num_classes)

        # domain discriminator--------------------------------------------------------------
        self.discriminator = Discriminator(2*args.dim_node)

        self.per_face_acc = []
        self.per_face_acc_target = []
        self.per_face_acc_target_known = []

    def training_step(self, batch, batch_idx, optimizer_idx):

        # =========================train the multi-binary classifier
        if optimizer_idx == 0:
            self.brep_encoder.eval()
            self.classifier.train()
            self.discriminator_p.train()
            self.discriminator_t.eval()
            self.discriminator.eval()

            # graph encoder-----------------------------------------------------------
            node_emb, graph_emb = self.brep_encoder(batch, last_state_only=True)  # graph_emb [batch_size, dim]

            # separate source-target data----------------------------------------------
            node_emb = node_emb[0].permute(1, 0, 2)  # node_emb [batch_size, max_node_num+1, dim] with global node
            node_emb = node_emb[:, 1:, :]            # node_emb [batch_size, max_node_num, dim] without global node
            node_emb_s, node_emb_t = node_emb.chunk(2, dim=0)
            graph_emb_s, graph_emb_t = graph_emb.chunk(2, dim=0)  # graph_emb [batch_size, dim]
            padding_mask_s, padding_mask_t = batch["padding_mask"].chunk(2, dim=0)  # [batch_size, max_node_num]
            node_pos_s = torch.where(padding_mask_s == False)  # [(batch_size, node_index)]
            local_feat_s = node_emb_s[node_pos_s]              # [total_nodes, dim]
            node_pos_t = torch.where(padding_mask_t == False)  # [(batch_size, node_index)]
            local_feat_t = node_emb_t[node_pos_t]              # [total_nodes, dim]

            # node feature extract------------------------------------------------------
            padding_mask_s_ = ~padding_mask_s
            num_nodes_per_graph_s = torch.sum(padding_mask_s_.long(), dim=-1)  # [batch_size]
            graph_emb_s_repeat = graph_emb_s.repeat_interleave(num_nodes_per_graph_s, dim=0).to(graph_emb.device)
            node_feat_s = torch.cat((local_feat_s, graph_emb_s_repeat), dim=1)  # node_feat_s [total_nodes, dim]
            padding_mask_t_ = ~padding_mask_t
            num_nodes_per_graph_t = torch.sum(padding_mask_t_.long(), dim=-1)      # [batch_size]
            graph_emb_t_repeat = graph_emb_t.repeat_interleave(num_nodes_per_graph_t, dim=0).to(graph_emb.device)
            node_feat_t = torch.cat((local_feat_t, graph_emb_t_repeat), dim=1)  # node_feat_t [total_nodes, dim]

            # source node classifier-------------------------------------------------------------
            node_seg_s = self.classifier(node_feat_s)  # [total_nodes, num_classes + 1]

            # discriminator_p--------------------------------------------------------------
            p0 = self.discriminator_p(node_feat_s)  # p0 [total_nodes, num_classes]

            # source classifier loss----------------------------------------------------------
            num_node_s = node_seg_s.size()[0]
            label_s = batch["label_feature"][:num_node_s].long() - 1
            label_s = F.one_hot(label_s, self.num_classes + 1)
            loss_s = CrossEntropyLoss(label_s, node_seg_s)
            self.log("train_loss_Gy", loss_s, on_step=False, on_epoch=True)

            # multi_classifier loss-------------------------------------------------------------
            loss_sp = BCELossForMultiClassification(label_s[:, 0:self.num_classes], p0)
            self.log("train_loss_Gc", loss_sp, on_step=False, on_epoch=True)

            loss = loss_s + loss_sp
            self.log("train_loss", loss, on_step=False, on_epoch=True)
            torch.cuda.empty_cache()
            return loss

        # =========================train the known/unknown discriminator
        if optimizer_idx == 1:
            self.brep_encoder.eval()
            self.classifier.train()
            self.discriminator_p.train()
            self.discriminator_t.train()
            self.discriminator.eval()

            # graph encoder-----------------------------------------------------------
            node_emb, graph_emb = self.brep_encoder(batch, last_state_only=True)  # graph_emb [batch_size, dim]

            # separate source-target data----------------------------------------------
            node_emb = node_emb[0].permute(1, 0, 2)  # node_emb [batch_size, max_node_num+1, dim] with global node
            node_emb = node_emb[:, 1:, :]  # node_emb [batch_size, max_node_num, dim] without global node
            node_emb_s, node_emb_t = node_emb.chunk(2, dim=0)
            graph_emb_s, graph_emb_t = graph_emb.chunk(2, dim=0)  # graph_emb [batch_size, dim]
            padding_mask_s, padding_mask_t = batch["padding_mask"].chunk(2, dim=0)  # [batch_size, max_node_num]
            node_pos_s = torch.where(padding_mask_s == False)  # [(batch_size, node_index)]
            local_feat_s = node_emb_s[node_pos_s]  # [total_nodes, dim]
            node_pos_t = torch.where(padding_mask_t == False)  # [(batch_size, node_index)]
            local_feat_t = node_emb_t[node_pos_t]  # [total_nodes, dim]

            # node feature extract------------------------------------------------------
            padding_mask_s_ = ~padding_mask_s
            num_nodes_per_graph_s = torch.sum(padding_mask_s_.long(), dim=-1)  # [batch_size]
            graph_emb_s_repeat = graph_emb_s.repeat_interleave(num_nodes_per_graph_s, dim=0).to(graph_emb.device)
            node_feat_s = torch.cat((local_feat_s, graph_emb_s_repeat), dim=1)  # node_feat_s [total_nodes, dim]
            padding_mask_t_ = ~padding_mask_t
            num_nodes_per_graph_t = torch.sum(padding_mask_t_.long(), dim=-1)  # [batch_size]
            graph_emb_t_repeat = graph_emb_t.repeat_interleave(num_nodes_per_graph_t, dim=0).to(graph_emb.device)
            node_feat_t = torch.cat((local_feat_t, graph_emb_t_repeat), dim=1)  # node_feat_t [total_nodes, dim]

            # node classifier-------------------------------------------------------------
            node_seg_s = self.classifier(node_feat_s)  # [total_nodes, num_classes + 1]

            # discriminator_p--------------------------------------------------------------
            p0 = self.discriminator_p(node_feat_s)  # p0 [total_nodes, num_classes]
            p1 = self.discriminator_p(node_feat_t)  # p1 [total_nodes, num_classes]
            p2 = torch.sum(p1, dim=-1)              # p2 [total_nodes]

            # discriminator_t--------------------------------------------------------------
            num_known_sample = int(0.02*p2.size()[0])
            w = torch.sort(p2.detach(), dim=0)[1][-num_known_sample:]  # known node index
            h = torch.sort(p2.detach(), dim=0)[1][:num_known_sample]   # unknown node index
            feature_otherep00 = torch.index_select(node_feat_t, 0, w.view(num_known_sample))
            feature_otherep01 = torch.index_select(node_feat_t, 0, h.view(num_known_sample))
            pred00 = self.discriminator_t(feature_otherep00)  # known
            pred01 = self.discriminator_t(feature_otherep01)  # unknown

            # source classifier loss----------------------------------------------------------
            num_node_s = node_seg_s.size()[0]
            label_s = batch["label_feature"][:num_node_s].long() - 1
            label_s = F.one_hot(label_s, self.num_classes + 1)
            loss_s = CrossEntropyLoss(label_s, node_seg_s)
            self.log("train_loss_Gy", loss_s, on_step=False, on_epoch=True)

            # multi_classifier loss-------------------------------------------------------------
            loss_sp = BCELossForMultiClassification(label_s[:, 0:self.num_classes], p0)
            self.log("train_loss_Gc", loss_sp, on_step=False, on_epoch=True)

            # known/unknown discriminator loss------------------------------------------------
            label_known = torch.from_numpy(np.concatenate((np.ones((num_known_sample, 1)), np.zeros((num_known_sample, 1))), axis=-1).astype('float32')).to(pred00.device)
            label_unknown = torch.from_numpy(np.concatenate((np.zeros((num_known_sample, 1)), np.ones((num_known_sample, 1))), axis=-1).astype('float32')).to(pred01.device)
            loss_tt = CrossEntropyLoss(label_known, pred00)
            loss_tt += CrossEntropyLoss(label_unknown, pred01)
            self.log("train_loss_Gb", loss_tt, on_step=False, on_epoch=True)

            loss = loss_s + loss_sp + loss_tt
            self.log("train_loss", loss, on_step=False, on_epoch=True)
            torch.cuda.empty_cache()
            return loss

        # =========================train domain discriminator with weight
        if optimizer_idx == 2:
            self.brep_encoder.train()
            self.classifier.train()
            self.discriminator_p.eval()
            self.discriminator_t.eval()
            self.discriminator.train()

            # graph encoder-----------------------------------------------------------
            node_emb, graph_emb = self.brep_encoder(batch, last_state_only=True)  # graph_emb [batch_size, dim]

            # separate source-target data----------------------------------------------
            node_emb = node_emb[0].permute(1, 0, 2)  # node_emb [batch_size, max_node_num+1, dim] with global node
            node_emb = node_emb[:, 1:, :]  # node_emb [batch_size, max_node_num, dim] without global node
            node_emb_s, node_emb_t = node_emb.chunk(2, dim=0)
            graph_emb_s, graph_emb_t = graph_emb.chunk(2, dim=0)  # graph_emb [batch_size, dim]
            padding_mask_s, padding_mask_t = batch["padding_mask"].chunk(2, dim=0)  # [batch_size, max_node_num]
            node_pos_s = torch.where(padding_mask_s == False)  # [(batch_size, node_index)]
            local_feat_s = node_emb_s[node_pos_s]  # [total_nodes, dim]
            node_pos_t = torch.where(padding_mask_t == False)  # [(batch_size, node_index)]
            local_feat_t = node_emb_t[node_pos_t]  # [total_nodes, dim]

            # node feature extract------------------------------------------------------
            padding_mask_s_ = ~padding_mask_s
            num_nodes_per_graph_s = torch.sum(padding_mask_s_.long(), dim=-1)  # [batch_size]
            graph_emb_s_repeat = graph_emb_s.repeat_interleave(num_nodes_per_graph_s, dim=0).to(graph_emb.device)
            node_feat_s = torch.cat((local_feat_s, graph_emb_s_repeat), dim=1)  # node_feat_s [total_nodes, dim]
            padding_mask_t_ = ~padding_mask_t
            num_nodes_per_graph_t = torch.sum(padding_mask_t_.long(), dim=-1)  # [batch_size]
            graph_emb_t_repeat = graph_emb_t.repeat_interleave(num_nodes_per_graph_t, dim=0).to(graph_emb.device)
            node_feat_t = torch.cat((local_feat_t, graph_emb_t_repeat), dim=1)  # node_feat_t [total_nodes, dim]

            # node classifier-------------------------------------------------------------
            node_seg_s = self.classifier(node_feat_s)  # [total_nodes, num_classes + 1]
            node_seg_t = self.classifier(node_feat_t)  # [total_nodes, num_classes + 1]

            domain_prob_source = self.discriminator(node_feat_s)  # [total_nodes, 1] source->1, target->0
            domain_prob_target = self.discriminator(node_feat_t)  # [total_nodes, 1]

            dp_target = self.discriminator_t(node_feat_t.detach())  # [total_nodes, 2]  as weight  {known, unknown}

            # source classify loss----------------------------------------------------------
            num_node_s = node_seg_s.size()[0]
            label_s = batch["label_feature"][:num_node_s].long() - 1
            label_s = F.one_hot(label_s, self.num_classes + 1)
            loss_s = CrossEntropyLoss(label_s, node_seg_s)
            self.log("train_loss_Gy", loss_s, on_step=False, on_epoch=True)

            # domain_adv loss----------------------------------------------------------------
            label_source = torch.ones_like(domain_prob_source, dtype=torch.float, device=domain_prob_source.device)
            label_target = torch.ones_like(domain_prob_target, dtype=torch.float, device=domain_prob_target.device)
            loss_adv = BCELossForMultiClassification(label_source, domain_prob_source)
            # loss_adv += BCELossForMultiClassification(label_target, (1-domain_prob_target), instance_level_weight=dp_target[:,0].contiguous())
            loss_adv += BCELossForMultiClassification(label_target, (1 - domain_prob_target))
            self.log("train_loss_Gd", loss_adv, on_step=False, on_epoch=True)

            # target entropy loss----------------------------------------------------------
            # loss_entropy = EntropyLoss(node_seg_t, instance_level_weight=dp_target[:, 0].contiguous())
            loss_entropy = EntropyLoss(node_seg_t)
            self.log("train_loss_en", loss_adv, on_step=False, on_epoch=True)

            loss = loss_s + 0.3*loss_adv + 0.1*loss_entropy
            self.log("train_loss", loss, on_step=False, on_epoch=True)
            torch.cuda.empty_cache()
            return loss

        # =========================train classifier with unknown-label
        if optimizer_idx == 3:
            self.brep_encoder.train()
            self.classifier.train()
            self.discriminator_p.eval()
            self.discriminator_t.eval()
            self.discriminator.train()

            # graph encoder-----------------------------------------------------------
            node_emb, graph_emb = self.brep_encoder(batch, last_state_only=True)  # graph_emb [batch_size, dim]

            # separate source-target data----------------------------------------------
            node_emb = node_emb[0].permute(1, 0, 2)  # node_emb [batch_size, max_node_num+1, dim] with global node
            node_emb = node_emb[:, 1:, :]  # node_emb [batch_size, max_node_num, dim] without global node
            node_emb_s, node_emb_t = node_emb.chunk(2, dim=0)
            graph_emb_s, graph_emb_t = graph_emb.chunk(2, dim=0)  # graph_emb [batch_size, dim]
            padding_mask_s, padding_mask_t = batch["padding_mask"].chunk(2, dim=0)  # [batch_size, max_node_num]
            node_pos_s = torch.where(padding_mask_s == False)  # [(batch_size, node_index)]
            local_feat_s = node_emb_s[node_pos_s]  # [total_nodes, dim]
            node_pos_t = torch.where(padding_mask_t == False)  # [(batch_size, node_index)]
            local_feat_t = node_emb_t[node_pos_t]  # [total_nodes, dim]

            # node feature extract------------------------------------------------------
            padding_mask_s_ = ~padding_mask_s
            num_nodes_per_graph_s = torch.sum(padding_mask_s_.long(), dim=-1)  # [batch_size]
            graph_emb_s_repeat = graph_emb_s.repeat_interleave(num_nodes_per_graph_s, dim=0).to(graph_emb.device)
            node_feat_s = torch.cat((local_feat_s, graph_emb_s_repeat), dim=1)  # node_feat_s [total_nodes, dim]
            padding_mask_t_ = ~padding_mask_t
            num_nodes_per_graph_t = torch.sum(padding_mask_t_.long(), dim=-1)  # [batch_size]
            graph_emb_t_repeat = graph_emb_t.repeat_interleave(num_nodes_per_graph_t, dim=0).to(graph_emb.device)
            node_feat_t = torch.cat((local_feat_t, graph_emb_t_repeat), dim=1)  # node_feat_t [total_nodes, dim]

            # node classifier-------------------------------------------------------------
            node_seg_s = self.classifier(node_feat_s)  # [total_nodes, num_classes + 1]
            node_seg_t = self.classifier(node_feat_t)  # [total_nodes, num_classes + 1]

            domain_prob_source = self.discriminator(node_feat_s)  # [total_nodes, 1] source->1, target->0
            domain_prob_target = self.discriminator(node_feat_t)  # [total_nodes, 1]

            dp_target = self.discriminator_t(node_feat_t.detach()) # [total_nodes, 2]  as weight {known, unknown}

            # select possible unknown_label sample
            num_unknown_sample = int(0.02 * node_feat_t.size()[0])
            r = torch.sort(dp_target[:, 1].detach(), dim=0)[1][-num_unknown_sample:]
            feature_otherep = torch.index_select(node_feat_t, 0, r.view(num_unknown_sample))
            unknown_node_seg_t = self.classifier(feature_otherep)    # [num_unknown_sample, num_classes + 1]

            # source classify loss----------------------------------------------------------
            num_node_s = node_seg_s.size()[0]
            label_s = batch["label_feature"][:num_node_s].long() - 1
            label_s = F.one_hot(label_s, self.num_classes + 1)
            loss_s = CrossEntropyLoss(label_s, node_seg_s)
            self.log("train_loss_Gy", loss_s, on_step=False, on_epoch=True)

            # domain_adv loss----------------------------------------------------------------
            label_source = torch.ones_like(domain_prob_source, dtype=torch.float, device=domain_prob_source.device)
            label_target = torch.ones_like(domain_prob_target, dtype=torch.float, device=domain_prob_source.device)
            loss_adv = BCELossForMultiClassification(label_source, domain_prob_source)
            # loss_adv += BCELossForMultiClassification(label_target, (1-domain_prob_target), instance_level_weight=dp_target[:, 0].contiguous())
            loss_adv += BCELossForMultiClassification(label_target, (1 - domain_prob_target))
            self.log("train_loss_Gd", loss_adv, on_step=False, on_epoch=True)

            # target entropy loss----------------------------------------------------------
            # loss_entropy = EntropyLoss(node_seg_t, instance_level_weight=dp_target[:, 0].contiguous())
            loss_entropy = EntropyLoss(node_seg_t)
            self.log("train_loss_en", loss_entropy, on_step=False, on_epoch=True)

            # target possible unknown label sample loss
            label_unknown = torch.from_numpy(np.concatenate((np.zeros((num_unknown_sample, self.num_classes)), np.ones((num_unknown_sample, 1))), axis=-1).astype('float32')).to(unknown_node_seg_t.device)
            loss_unk = CrossEntropyLoss(label_unknown, unknown_node_seg_t)
            self.log("train_loss_unk", loss_unk, on_step=False, on_epoch=True)

            # loss = loss_s + 0.3*loss_adv + 0.1*loss_entropy + 0.1*loss_unk
            loss = loss_s + 0.3 * loss_adv + 0.1 * loss_entropy
            self.log("train_loss", loss, on_step=False, on_epoch=True)
            torch.cuda.empty_cache()
            return loss


    def validation_step(self, batch, batch_idx):
        self.brep_encoder.eval()
        self.classifier.eval()
        self.discriminator_p.eval()
        self.discriminator_t.eval()
        self.discriminator.eval()

        # graph encoder-----------------------------------------------------------
        node_emb, graph_emb = self.brep_encoder(batch, last_state_only=True)  # graph_emb [batch_size, dim]

        # separate source-target data----------------------------------------------
        node_emb = node_emb[0].permute(1, 0, 2)  # node_emb [batch_size, max_node_num+1, dim] with global node
        node_emb = node_emb[:, 1:, :]  # node_emb [batch_size, max_node_num, dim] without global node
        node_emb_s, node_emb_t = node_emb.chunk(2, dim=0)
        graph_emb_s, graph_emb_t = graph_emb.chunk(2, dim=0)  # graph_emb [batch_size, dim]
        padding_mask_s, padding_mask_t = batch["padding_mask"].chunk(2, dim=0)  # [batch_size, max_node_num]
        node_pos_s = torch.where(padding_mask_s == False)  # [(batch_size, node_index)]
        local_feat_s = node_emb_s[node_pos_s]  # [total_nodes, dim]
        node_pos_t = torch.where(padding_mask_t == False)  # [(batch_size, node_index)]
        local_feat_t = node_emb_t[node_pos_t]  # [total_nodes, dim]

        # node feature extract------------------------------------------------
        padding_mask_s_ = ~padding_mask_s
        num_nodes_per_graph_s = torch.sum(padding_mask_s_.long(), dim=-1)  # [batch_size]
        graph_emb_s_repeat = graph_emb_s.repeat_interleave(num_nodes_per_graph_s, dim=0).to(graph_emb.device)
        node_feat_s = torch.cat((local_feat_s, graph_emb_s_repeat), dim=1)  # node_feat_s [total_nodes, dim]

        padding_mask_t_ = ~padding_mask_t
        num_nodes_per_graph_t = torch.sum(padding_mask_t_.long(), dim=-1)  # [batch_size]
        graph_emb_t_repeat = graph_emb_t.repeat_interleave(num_nodes_per_graph_t, dim=0).to(graph_emb.device)
        node_feat_t = torch.cat((local_feat_t, graph_emb_t_repeat), dim=1)  # node_feat_t [total_nodes, dim]

        # node classifier--------------------------------------------------------------
        node_seg_s = self.classifier(node_feat_s)  # [total_nodes, num_classes + 1]
        node_seg_t = self.classifier(node_feat_t)  # [total_nodes, num_classes + 1]

        # source classify loss---------------------------------------------------------
        num_node_s = node_seg_s.size()[0]
        label_s = batch["label_feature"][:num_node_s].long() - 1
        label_s_ = F.one_hot(label_s, self.num_classes + 1)
        loss_s = CrossEntropyLoss(label_s_, node_seg_s)
        self.log("eval_loss_s", loss_s, on_step=False, on_epoch=True)
        # pre_face_acc-----------------------------------
        pred_s = torch.argmax(node_seg_s, dim=-1)  # pres [total_nodes]
        pred_s_np = pred_s.long().detach().cpu().numpy()
        label_s_np = label_s.long().detach().cpu().numpy()
        per_face_comp = (pred_s_np == label_s_np).astype(np.int)
        self.per_face_acc.append(np.mean(per_face_comp))

        # target pre_face_acc-----------------------------------
        pred_t = torch.argmax(node_seg_t, dim=-1)  # pres [total_nodes]
        num_node_s = node_seg_s.size()[0]
        label_t = batch["label_feature"][num_node_s:]  # labels [total_nodes]
        label_unk = (self.num_classes + 1) * torch.ones_like(label_t)
        label_t = torch.where(label_t > self.num_classes, label_unk, label_t) - 1
        pred_t_np = pred_t.long().detach().cpu().numpy()
        label_t_np = label_t.long().detach().cpu().numpy()
        per_face_comp = (pred_t_np == label_t_np).astype(np.int)
        self.per_face_acc_target.append(np.mean(per_face_comp))

        loss = loss_s
        self.log("eval_loss", loss, on_step=False, on_epoch=True)
        torch.cuda.empty_cache()
        return loss

    def validation_epoch_end(self, val_step_outputs):
        self.log("per_face_accuracy", np.mean(self.per_face_acc))
        self.log("per_face_accuracy_target", np.mean(self.per_face_acc_target))
        self.per_face_acc = []
        self.per_face_acc_target = []


    def configure_optimizers(self):
        # step-1-1
        opt_step_1_1 = torch.optim.AdamW(self.classifier.parameters(), lr=0.005, betas=(0.99, 0.999))
        opt_step_1_1.add_param_group({'params': self.discriminator_p.parameters(), 'lr': 0.005, 'betas': (0.99, 0.999)})

        # step-1-2
        opt_step_1_2 = torch.optim.AdamW(self.classifier.parameters(), lr=0.005, betas=(0.99, 0.999))
        opt_step_1_2.add_param_group({'params': self.discriminator_p.parameters(), 'lr': 0.005, 'betas': (0.99, 0.999)})
        opt_step_1_2.add_param_group({'params': self.discriminator_t.parameters(), 'lr': 0.005, 'betas': (0.99, 0.999)})

        # step-2-1
        opt_step_2_1 = torch.optim.AdamW(self.brep_encoder.parameters(), lr=0.005, betas=(0.99, 0.999))
        opt_step_2_1.add_param_group({'params': self.classifier.parameters(), 'lr': 0.005, 'betas': (0.99, 0.999)})
        opt_step_2_1.add_param_group({'params': self.discriminator.parameters(), 'lr': 0.005, 'betas': (0.99, 0.999)})

        # step-2-2
        opt_step_2_2 = torch.optim.AdamW(self.brep_encoder.parameters(), lr=0.005, betas=(0.99, 0.999))
        opt_step_2_2.add_param_group({'params': self.classifier.parameters(), 'lr': 0.005, 'betas': (0.99, 0.999)})
        opt_step_2_2.add_param_group({'params': self.discriminator.parameters(), 'lr': 0.005, 'betas': (0.99, 0.999)})

        return (
            {'optimizer': opt_step_1_1, 'frequency': 1},  # step 1
            {'optimizer': opt_step_1_2, 'frequency': 1},  # step 1
            {'optimizer': opt_step_2_1, 'frequency': 500},  # step 5
            {'optimizer': opt_step_2_2, 'frequency': 100}   # step 1
        )